# -*- coding: utf-8 -*-
"""electra 모델학습.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1skKl_yeBFVHP9caK7FI0EB1wOndjlbQ-
"""

import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
from sklearn.metrics import precision_score, recall_score, f1_score

model = AutoModelForSequenceClassification.from_pretrained("daekeun-ml/koelectra-small-v3-nsmc")
tokenizer = AutoTokenizer.from_pretrained("daekeun-ml/koelectra-small-v3-nsmc")

from google.colab import drive
drive.mount('/content/drive')

# 주어진 텍스트를 토크나이징합니다.
text = "너무 맛있어요. 다음에 또 올게요"
tokens = tokenizer.tokenize(text)

print(tokens)

df = pd.read_excel('file.xlsx')

# 가장 긴 데이터의 길이 구하기
max_length = df.apply(lambda x: len(str(x).strip()), axis=1).max()

print("가장 긴 데이터의 길이:", max_length)

"""#학습시작"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 데이터셋 클래스 생성

class ReviewDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sentence = str(self.data.loc[idx, 'Sentence'])
        label = self.data.loc[idx, 'Label']

        encoding = self.tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=self.max_length,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

# 데이터셋 및 데이터로더 생성
MAX_LENGTH = 150  # 문장의 최대 길이 (256에서 바꿈)
BATCH_SIZE = 75 # 50  ~ 75

val_df = pd.read_excel('/content/drive/MyDrive/DSL/DSL_modeling_project/val_맞춤법.xlsx')
train_df = pd.read_excel('/content/drive/MyDrive/DSL/DSL_modeling_project/train_맞춤법.xlsx')

train_dataset = ReviewDataset(train_df, tokenizer, MAX_LENGTH)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

val_dataset = ReviewDataset(val_df, tokenizer, MAX_LENGTH)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 검증 데이터셋 레이블 가져오기
val_targets = val_df['Label'].values

# 모델 및 옵티마이저 설정
model = AutoModelForSequenceClassification.from_pretrained("daekeun-ml/koelectra-small-v3-nsmc") #모델 초기화 후 다시
model.to(device)
optimizer = AdamW(model.parameters(), lr=3e-5)
criterion = nn.CrossEntropyLoss()

EPOCHS = 20

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0.0
    correct_train = 0
    total_train = 0

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

        _, predicted = torch.max(outputs.logits, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    train_accuracy = correct_train / total_train

    # 검증
    model.eval()
    val_loss = 0.0
    correct_val = 0
    total_val = 0
    val_predictions = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            _, predicted = torch.max(outputs.logits, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()

            val_loss += loss.item()

            val_predictions.extend(predicted.tolist())

    val_accuracy = correct_val / total_val
    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)

    # 정밀도, 재현율, F1 점수 계산
    val_precision = precision_score(val_targets, val_predictions, average='weighted')
    val_recall = recall_score(val_targets, val_predictions, average='weighted')
    val_f1 = f1_score(val_targets, val_predictions, average='weighted')

    print(f'Epoch {epoch + 1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1:.4f}')

print("Training finished.")

from google.colab import drive
import os

# 모델을 저장할 경로
model_save_path = '/content/drive/MyDrive/DSL/DSL_modeling_project'

# 폴더가 존재하지 않으면 생성
os.makedirs(model_save_path, exist_ok=True)

# 모델 저장
model.save_pretrained(model_save_path)

print("모델 저장이 완료되었습니다.")